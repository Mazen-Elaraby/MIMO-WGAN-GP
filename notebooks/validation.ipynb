{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from src.model_builder import Generator, Critic\n",
    "from eval.eval import *\n",
    "from src.model_utils import *\n",
    "from src.data_setup import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation params\n",
    "L = 23\n",
    "Nr = Nt = 4\n",
    "NUM_ANTENNA_PAIRS = Nr * Nt\n",
    "z_dim = 50\n",
    "EMBED_DIM = 4\n",
    "HIDDEN_DIM = 100\n",
    "BATCH_SIZE = 12000\n",
    "T = 128\n",
    "N_CRITIC = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (embedding): Linear(in_features=16, out_features=4, bias=True)\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=54, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=100, out_features=46, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "g = Generator(Nr, Nt, L, z_dim, EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "g = g.double()\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(os.path.join('models', 'G_B0L1_BSZ512_EMB4_Z50.pt'))\n",
    "g.load_state_dict(state_dict)\n",
    "\n",
    "g.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Critic(\n",
       "  (embedding): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=268, out_features=100, bias=True)\n",
       "    (1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (4): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Critic(T+Nr, Nr).to(device)\n",
    "c = c.double()\n",
    "state_dict = torch.load(os.path.join('models', 'C_B0L1_BSZ512_EMB4_Z50.pt'))\n",
    "c.load_state_dict(state_dict)\n",
    "\n",
    "c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_WGAN_GP(generator: nn.Module,\n",
    "                  critic: nn.Module,\n",
    "                  train_dataloader: torch.utils.data.DataLoader, \n",
    "                  val_dataloader: torch.utils.data.DataLoader, \n",
    "                  sample_indices: torch.Tensor,\n",
    "                  epochs: int,\n",
    "                  device: torch.device) -> Dict[str, List]:\n",
    "\n",
    "    # setting up transmitted signal - unit power discrete impulse\n",
    "    input_signal = torch.zeros(1, Nt, T, device=device, dtype=torch.complex128)\n",
    "    input_signal[0,0,0] = 1\n",
    "    input_signal[0,1,12] = 1 #12\n",
    "    input_signal[0,2,25] = 1 #25\n",
    "    input_signal[0,3,39] = 1 #39\n",
    "\n",
    "    ij_matrix_full = torch.eye(NUM_ANTENNA_PAIRS, dtype=torch.float64, device=device).repeat(BATCH_SIZE, 1) \n",
    "    i_matrix_full = torch.eye(Nr, dtype=torch.float64, device=device).repeat(BATCH_SIZE, 1)\n",
    "\n",
    "    generator.train()\n",
    "    critic.train()\n",
    "\n",
    "    c_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        c_loss = g_loss = 0\n",
    "        for batch_idx, batch_real in enumerate(tqdm(train_dataloader)):\n",
    "            \n",
    "            batch_real = batch_real.to(device) # shape [BATCH_SIZE, Nr, T+Nr]\n",
    "            cur_batch_size = batch_real.shape[0]\n",
    "            # setting up conditioning information\n",
    "            ij_matrix = ij_matrix_full[:(cur_batch_size*NUM_ANTENNA_PAIRS)]\n",
    "            i_matrix = i_matrix_full[:(cur_batch_size*Nr)] \n",
    "\n",
    "            # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "            # equivalent to minimizing the negative of that\n",
    "            for _ in range(N_CRITIC):\n",
    "\n",
    "                # generating a batch of fake data\n",
    "                z = torch.randn(cur_batch_size*NUM_ANTENNA_PAIRS, z_dim, dtype=torch.float64, device=device)\n",
    "\n",
    "                channel_tensor = generator(z, ij_matrix)\n",
    "                batch_fake = get_fake_batch(input_signal, channel_tensor, sample_indices)\n",
    "                \n",
    "                # interleave real and imaginary\n",
    "                batch_real_int = prepare_complex_signal(batch_real).view(cur_batch_size*Nr, -1)\n",
    "                batch_fake_int = prepare_complex_signal(batch_fake).view(cur_batch_size*Nr, -1)\n",
    "\n",
    "                # calculating critic loss\n",
    "                critic_real = critic(batch_real_int, i_matrix).view(-1)\n",
    "                critic_fake = critic(batch_fake_int, i_matrix).view(-1)\n",
    "                gp = gradient_penalty(critic, batch_real, batch_fake, i_matrix)\n",
    "                critic_loss = (-(torch.mean(critic_real) - torch.mean(critic_fake))) + (10 * gp)\n",
    "                c_loss += critic_loss.item()\n",
    "\n",
    "            # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "            gen_fake = critic(batch_fake_int, i_matrix).view(-1)\n",
    "            gen_loss = -(torch.mean(gen_fake))\n",
    "            g_loss += gen_loss.item()\n",
    "\n",
    "        c_loss = c_loss / (N_CRITIC * len(train_dataloader))\n",
    "        g_loss = g_loss / len(train_dataloader)\n",
    "        c_losses.append(c_loss)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \\ Loss D: {c_loss:.4f}, loss G: {g_loss:.4f}\")\n",
    "\n",
    "    return c_losses, g_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3499725c17540388763e640428ef5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# train models\u001b[39;00m\n\u001b[0;32m     18\u001b[0m sample_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m35\u001b[39m, \u001b[38;5;241m38\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m44\u001b[39m, \u001b[38;5;241m46\u001b[39m, \u001b[38;5;241m49\u001b[39m, \u001b[38;5;241m89\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice) \n\u001b[1;32m---> 19\u001b[0m c_losses, g_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_WGAN_GP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mtrain_WGAN_GP\u001b[1;34m(generator, critic, train_dataloader, val_dataloader, sample_indices, epochs, device)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_CRITIC):\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# generating a batch of fake data\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(cur_batch_size\u001b[38;5;241m*\u001b[39mNUM_ANTENNA_PAIRS, z_dim, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 47\u001b[0m     channel_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mij_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     batch_fake \u001b[38;5;241m=\u001b[39m get_fake_batch(input_signal, channel_tensor, sample_indices)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# interleave real and imaginary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mazen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\GP\\sem 2\\MIMO GAN\\project\\src\\model_builder.py:48\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, z, ij)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z: torch\u001b[38;5;241m.\u001b[39mTensor, ij: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    Forward pass of the generator.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Batch of generated channel vectors.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     ij_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mij\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     combined_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z, ij_embedded), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain(combined_input)\n",
      "File \u001b[1;32mc:\\Users\\Mazen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Mazen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup directories\n",
    "train_dataset_path = os.path.join(\"Dataset\", \"train_data_TDL_A.mat\")\n",
    "test_dataset_path = os.path.join(\"Dataset\", \"test_data_TDL_A.mat\")\n",
    "val_dataset_path = os.path.join(\"Dataset\", \"val_data_TDL_A.mat\")\n",
    "\n",
    "train_dataloader, _, val_dataloader = create_dataloaders(train_dataset_path, test_dataset_path, val_dataset_path,\n",
    "                                                                    \"rx_train_data\", \"rx_test_data\", \"rx_val_data\",\n",
    "                                                                        BATCH_SIZE, 0)\n",
    "\n",
    "# Intialize Models (Generator & Critic)\n",
    "generator = Generator(Nr=Nr, Nt=Nt, l=L, z_dim=z_dim, embed_dim=EMBED_DIM).to(device)\n",
    "generator = generator.double()  # Converts all parameters to torch.float64\n",
    "\n",
    "critic = Critic(N=T+Nr, num_receive_antennas=Nr, embed_dim=EMBED_DIM).to(device)\n",
    "critic = critic.double() # Converts all parameters to torch.float64\n",
    "\n",
    "# train models\n",
    "sample_indices = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 14, 17, 19, 20, 22, 23, 28, 35, 38, 42, 44, 46, 49, 89], device=device) \n",
    "c_losses, g_losses = train_WGAN_GP(g, c, train_dataloader=val_dataloader, val_dataloader=val_dataloader, sample_indices=sample_indices, epochs=500, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
